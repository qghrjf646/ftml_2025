{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e26345a5",
   "metadata": {},
   "source": [
    "# Exercise 3: Expected Value of Empirical Risk for OLS\n",
    "\n",
    "## Question 1 (M): Compare the value in Proposition 1 to the Bayes risk.\n",
    "\n",
    "Proposition 1 states that $\\mathbb{E}[R_X(\\hat{\\theta})] = \\frac{n-d}{n} \\sigma^2$. The Bayes risk for the fixed design is $\\sigma^2$. Since $\\frac{n-d}{n} < 1$, we have $\\mathbb{E}[R_X(\\hat{\\theta})] < \\sigma^2$. The expected empirical risk underestimates the true risk, and the gap increases as $d$ increases relative to $n$. Meaning that the bigger $d$ the smaller the expectance of the fixed design risk, hence the better the estimation.\n",
    "\n",
    "---\n",
    "\n",
    "## Question 2 (M): Show that\n",
    "$$\n",
    "\\mathbb{E}[R_n(\\hat{\\theta})] = \\mathbb{E}_\\varepsilon \\left[ \\frac{1}{n} \\| (I_n - X(X^T X)^{-1} X^T) \\varepsilon \\|^2 \\right]\n",
    "$$\n",
    "\n",
    "**Clarification:**\n",
    "- $\\theta^*$ is the true parameter (unknown).\n",
    "- $\\hat{\\theta}$ is the OLS estimator: $\\hat{\\theta} = (X^T X)^{-1} X^T y$.\n",
    "- $y = X\\theta^* + \\varepsilon$.\n",
    "\n",
    "The empirical risk is:\n",
    "$$\n",
    "R_n(\\hat{\\theta}) = \\frac{1}{n} \\| y - X\\hat{\\theta} \\|^2\n",
    "$$\n",
    "Plug in the expressions:\n",
    "$$\n",
    "X\\hat{\\theta} = X(X^T X)^{-1} X^T y\n",
    "$$\n",
    "So,\n",
    "$$\n",
    "y - X\\hat{\\theta} = y - X(X^T X)^{-1} X^T y = (I_n - X(X^T X)^{-1} X^T) y\n",
    "$$\n",
    "Now, substitute $y = X\\theta^* + \\varepsilon$:\n",
    "$$\n",
    "y - X\\hat{\\theta} = (I_n - X(X^T X)^{-1} X^T)(X\\theta^* + \\varepsilon)\n",
    "$$\n",
    "Expand:\n",
    "$$\n",
    "= (I_n - X(X^T X)^{-1} X^T)X\\theta^* + (I_n - X(X^T X)^{-1} X^T)\\varepsilon\n",
    "$$\n",
    "Now, note that:\n",
    "$$\n",
    "(I_n - X(X^T X)^{-1} X^T)X = X - X(X^T X)^{-1} X^T X = X - X = 0\n",
    "$$\n",
    "because $X^T X (X^T X)^{-1} = I_d$.\n",
    "\n",
    "Therefore,\n",
    "$$\n",
    "y - X\\hat{\\theta} = (I_n - X(X^T X)^{-1} X^T)\\varepsilon\n",
    "$$\n",
    "So,\n",
    "$$\n",
    "R_n(\\hat{\\theta}) = \\frac{1}{n} \\| (I_n - X(X^T X)^{-1} X^T)\\varepsilon \\|^2\n",
    "$$\n",
    "Taking the expectation over $\\varepsilon$ gives the result.\n",
    "\n",
    "---\n",
    "\n",
    "## Question 3 (M): Let $A \\in \\mathbb{R}^{n \\times n}$. Show that $\\sum_{i,j} A_{ij}^2 = \\operatorname{tr}(A^T A)$\n",
    "\n",
    "By definition,\n",
    "$$\n",
    "\\operatorname{tr}(A^T A) = \\sum_{k=1}^n (A^T A)_{kk} = \\sum_{k=1}^n \\sum_{l=1}^n A_{lk} A_{kl} = \\sum_{k=1}^n \\sum_{l=1}^n A_{lk} A_{lk} = \\sum_{l=1}^n \\sum_{k=1}^n A_{lk}^2 = \\sum_{i=1}^n \\sum_{j=1}^n A_{ij}^2\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Question 4 (M): Show that $\\mathbb{E}_\\varepsilon \\left[ \\frac{1}{n} \\|A\\varepsilon\\|^2 \\right] = \\frac{\\sigma^2}{n} \\operatorname{tr}(A^T A)$\n",
    "\n",
    "Let $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I_n)$. Then:\n",
    "- $\\mathbb{E}[\\varepsilon] = 0$ (mean zero)\n",
    "- $\\operatorname{Cov}(\\varepsilon) = \\mathbb{E}[(\\varepsilon - \\mathbb{E}[\\varepsilon])(\\varepsilon - \\mathbb{E}[\\varepsilon])^T] = \\sigma^2 I_n$\n",
    "- Thus, $\\mathbb{E}[\\varepsilon \\varepsilon^T] = \\operatorname{Cov}(\\varepsilon) = \\sigma^2 I_n$\n",
    "\n",
    "Now, expand the quadratic form:\n",
    "$$\n",
    "\\|A\\varepsilon\\|^2 = (A\\varepsilon)^T (A\\varepsilon) = \\varepsilon^T A^T A \\varepsilon\n",
    "$$\n",
    "\n",
    "The expectation of a quadratic form is:\n",
    "$$\n",
    "\\mathbb{E}_\\varepsilon[\\varepsilon^T B \\varepsilon] = \\operatorname{tr}(B \\mathbb{E}[\\varepsilon \\varepsilon^T])\n",
    "$$\n",
    "for any symmetric matrix $B$. (Recall: $A^T_{ji} = A_{ij}$.)\n",
    "\n",
    "Apply this to $B = A^T A$:\n",
    "$$\n",
    "\\mathbb{E}_\\varepsilon[\\varepsilon^T A^T A \\varepsilon] = \\operatorname{tr}(A^T A \\mathbb{E}[\\varepsilon \\varepsilon^T]) = \\operatorname{tr}(A^T A \\sigma^2 I_n) = \\sigma^2 \\operatorname{tr}(A^T A)\n",
    "$$\n",
    "\n",
    "So\n",
    "$$\n",
    "\\mathbb{E}_\\varepsilon \\left[ \\frac{1}{n} \\|A\\varepsilon\\|^2 \\right] = \\frac{\\sigma^2}{n} \\operatorname{tr}(A^T A)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Question 5 (M): $A = I_n - X(X^T X)^{-1} X^T$. Show that $A^TA = A$.\n",
    "\n",
    "$A$ is symmetric because $A^T = (I_n - X(X^T X)^{-1} X^T)^T = I_n - X(X^T X)^{-1} X^T = A$.\n",
    "$A$ is idempotent:\n",
    "$$\n",
    "A^2 = (I_n - X(X^T X)^{-1} X^T)^2 = I_n - 2X(X^T X)^{-1} X^T + X(X^T X)^{-1} X^T X(X^T X)^{-1} X^T\n",
    "$$\n",
    "But $X^T X$ is invertible and $X^T X (X^T X)^{-1} = I_d$, so $X^T X(X^T X)^{-1} X^T = X^T$.\n",
    "\n",
    "Thus, $X(X^T X)^{-1} X^T X(X^T X)^{-1} X^T = X(X^T X)^{-1} X^T$.\n",
    "\n",
    "So $A^TA = A^2 = I_n - 2X(X^T X)^{-1} X^T + X(X^T X)^{-1} X^T X(X^T X)^{-1} X^T = A$.\n",
    "\n",
    "---\n",
    "\n",
    "## Question 6 (M): Conclusion: we show that $\\operatorname{tr}(A) = n - d$.\n",
    "\n",
    "Recall $A = I_n - X(X^T X)^{-1} X^T$.\n",
    "\n",
    "First, use the linearity of the trace:\n",
    "$$\n",
    "\\operatorname{tr}(A) = \\operatorname{tr}(I_n) - \\operatorname{tr}(X(X^T X)^{-1} X^T)\n",
    "$$\n",
    "\n",
    "Recall the cyclic property of the trace: $\\operatorname{tr}(AB) = \\operatorname{tr}(BA)$ for compatible matrices $A, B$.\n",
    "\n",
    "So,\n",
    "$$\n",
    "\\operatorname{tr}(X(X^T X)^{-1} X^T) = \\operatorname{tr}((X^T X)^{-1} X^T X) = \\operatorname{tr}(I_d) = d\n",
    "$$\n",
    "\n",
    "Also, $\\operatorname{tr}(I_n) = n$.\n",
    "\n",
    "Therefore,\n",
    "$$\n",
    "\\operatorname{tr}(A) = n - d\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Question 7 (M): What is $\\mathbb{E}\\left[ \\frac{\\|y - X\\hat{\\theta}\\|^2}{n-d} \\right]$?\n",
    "\n",
    "Recall from Question 2 that\n",
    "$$\n",
    "R_n(\\hat{\\theta}) = \\frac{1}{n} \\|y - X\\hat{\\theta}\\|^2\n",
    "$$\n",
    "so\n",
    "$$\n",
    "\\|y - X\\hat{\\theta}\\|^2 = n R_n(\\hat{\\theta})\n",
    "$$\n",
    "From Question 4, we have\n",
    "$$\n",
    "\\mathbb{E}[R_n(\\hat{\\theta})] = \\frac{n-d}{n} \\sigma^2\n",
    "$$\n",
    "Therefore,\n",
    "$$\n",
    "\\mathbb{E}[\\|y - X\\hat{\\theta}\\|^2] = n \\cdot \\mathbb{E}[R_n(\\hat{\\theta})] = n \\cdot \\frac{n-d}{n} \\sigma^2 = (n-d)\\sigma^2\n",
    "$$\n",
    "So,\n",
    "$$\n",
    "\\mathbb{E}\\left[ \\frac{\\|y - X\\hat{\\theta}\\|^2}{n-d} \\right] = \\sigma^2\n",
    "$$\n",
    "\n",
    "Recall: An estimator $T$ of a parameter $\\theta$ is called unbiased if $\\mathbb{E}[T] = \\theta$.\n",
    "\n",
    "**Conclusion:**\n",
    "$\\frac{\\|y - X\\hat{\\theta}\\|^2}{n-d}$ is an unbiased estimator of $\\sigma^2$ in the fixed design OLS setting, because its expectation is exactly $\\sigma^2$.\n",
    "\n",
    "---\n",
    "\n",
    "## Question 8 (C): Simulation to estimate σ²\n",
    "\n",
    "**Produce a numerical simulation that estimates σ² thanks to the result from Question 7. Check that the result is consistent with the theoretical value.**\n",
    "\n",
    "From Question 7, we showed that $\\mathbb{E}\\left[ \\frac{\\|y - X\\hat{\\theta}\\|^2}{n-d} \\right] = \\sigma^2$, meaning that $\\frac{\\|y - X\\hat{\\theta}\\|^2}{n-d}$ is an unbiased estimator of $\\sigma^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cab48f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Exercise 3, Question 8: Simulation ===\n",
      "Verifying that ||y - X*hat_theta||^2 / (n-d) is an unbiased estimator of sigma^2\n",
      "\n",
      "Simulation parameters:\n",
      "Number of simulations: 1000\n",
      "Sample size (n): 100\n",
      "Number of features (d): 5\n",
      "True σ²: 6.25\n",
      "True parameter θ*: [ 1.5 -2.   0.5  3.  -1. ]\n",
      "\n",
      "Running 1000 simulations...\n",
      "\n",
      "=== Results ===\n",
      "True σ²: 6.2500\n",
      "Mean estimated σ²: 6.2735\n",
      "Standard deviation of estimates: 0.8948\n",
      "Bias: 0.0235\n",
      "\n",
      "Empirical risk comparison:\n",
      "Theoretical E[R_n(hat_theta)]: 5.9375\n",
      "Observed mean empirical risk: 5.9598\n",
      "\n",
      "Verification:\n",
      "✓ Unbiased estimator: |bias| < 0.1: True\n",
      "✓ Empirical risk matches theory: |difference| < 0.1: True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "print(\"=== Exercise 3, Question 8: Simulation ===\")\n",
    "print(\"Verifying that ||y - X*hat_theta||^2 / (n-d) is an unbiased estimator of sigma^2\")\n",
    "\n",
    "# Simulation parameters\n",
    "n_simulations = 1000\n",
    "n = 100  # number of samples\n",
    "d = 5    # number of features\n",
    "sigma_true = 2.5  # true noise standard deviation\n",
    "\n",
    "print(f\"\\nSimulation parameters:\")\n",
    "print(f\"Number of simulations: {n_simulations}\")\n",
    "print(f\"Sample size (n): {n}\")\n",
    "print(f\"Number of features (d): {d}\")\n",
    "print(f\"True σ²: {sigma_true**2}\")\n",
    "\n",
    "# Generate fixed design matrix X (deterministic)\n",
    "np.random.seed(123)  # Seed for generating the fixed design matrix X\n",
    "X = np.random.randn(n, d)\n",
    "\n",
    "# True parameter vector\n",
    "theta_star = np.array([1.5, -2.0, 0.5, 3.0, -1.0])\n",
    "\n",
    "print(f\"True parameter θ*: {theta_star}\")\n",
    "\n",
    "# Set seed for the simulation loop (for noise generation)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Storage for estimates\n",
    "sigma2_estimates = []\n",
    "empirical_risks = []\n",
    "\n",
    "print(f\"\\nRunning {n_simulations} simulations...\")\n",
    "\n",
    "for sim in range(n_simulations):\n",
    "    # Generate noise epsilon ~ N(0, σ²I_n)\n",
    "    epsilon = np.random.normal(0, sigma_true, n)\n",
    "    \n",
    "    # Generate observations: y = X*theta_star + epsilon\n",
    "    y = X @ theta_star + epsilon\n",
    "    \n",
    "    # Compute OLS estimator: hat_theta = (X^T X)^(-1) X^T y\n",
    "    XtX_inv = np.linalg.inv(X.T @ X)\n",
    "    theta_hat = XtX_inv @ X.T @ y\n",
    "    \n",
    "    # Compute residuals: y - X*hat_theta\n",
    "    residuals = y - X @ theta_hat\n",
    "    \n",
    "    # Estimate σ² using the unbiased estimator\n",
    "    sigma2_hat = np.sum(residuals**2) / (n - d)\n",
    "    sigma2_estimates.append(sigma2_hat)\n",
    "    \n",
    "    # Also compute empirical risk for comparison\n",
    "    empirical_risk = np.sum(residuals**2) / n\n",
    "    empirical_risks.append(empirical_risk)\n",
    "\n",
    "# Convert to numpy arrays\n",
    "sigma2_estimates = np.array(sigma2_estimates)\n",
    "empirical_risks = np.array(empirical_risks)\n",
    "\n",
    "# Compute statistics\n",
    "mean_sigma2_estimate = np.mean(sigma2_estimates)\n",
    "mean_empirical_risk = np.mean(empirical_risks)\n",
    "\n",
    "print(f\"\\n=== Results ===\")\n",
    "print(f\"True σ²: {sigma_true**2:.4f}\")\n",
    "print(f\"Mean estimated σ²: {mean_sigma2_estimate:.4f}\")\n",
    "print(f\"Standard deviation of estimates: {std_sigma2_estimate:.4f}\")\n",
    "print(f\"Bias: {mean_sigma2_estimate - sigma_true**2:.4f}\")\n",
    "\n",
    "# Compare with empirical risk\n",
    "theoretical_empirical_risk = ((n - d) / n) * sigma_true**2\n",
    "print(f\"\\nEmpirical risk comparison:\")\n",
    "print(f\"Theoretical E[R_n(hat_theta)]: {theoretical_empirical_risk:.4f}\")\n",
    "print(f\"Observed mean empirical risk: {mean_empirical_risk:.4f}\")\n",
    "\n",
    "print(f\"\\nVerification:\")\n",
    "print(f\"✓ Unbiased estimator: |bias| < 0.1: {abs(mean_sigma2_estimate - sigma_true**2) < 0.1}\")\n",
    "print(f\"✓ Empirical risk matches theory: |difference| < 0.1: {abs(mean_empirical_risk - theoretical_empirical_risk) < 0.1}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
