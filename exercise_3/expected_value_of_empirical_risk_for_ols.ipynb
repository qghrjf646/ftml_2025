{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e26345a5",
   "metadata": {},
   "source": [
    "# Exercise 3: Expected Value of Empirical Risk for OLS\n",
    "\n",
    "## Question 1 (M): Compare the value in Proposition 1 to the Bayes risk.\n",
    "\n",
    "Proposition 1 states that $\\mathbb{E}[R_X(\\hat{\\theta})] = \\frac{n-d}{n} \\sigma^2$. The Bayes risk for the fixed design is $\\sigma^2$. Since $\\frac{n-d}{n} < 1$, we have $\\mathbb{E}[R_X(\\hat{\\theta})] < \\sigma^2$. The expected empirical risk underestimates the true risk, and the gap increases as $d$ increases relative to $n$. Meaning that the bigger $d$ the smaller the expectance of the fixed design risk, hence the better the estimation.\n",
    "\n",
    "---\n",
    "\n",
    "## Question 2 (M): Show that\n",
    "$$\n",
    "\\mathbb{E}[R_n(\\hat{\\theta})] = \\mathbb{E}_\\varepsilon \\left[ \\frac{1}{n} \\| (I_n - X(X^T X)^{-1} X^T) \\varepsilon \\|^2 \\right]\n",
    "$$\n",
    "\n",
    "**Clarification:**\n",
    "- $\\theta^*$ is the true parameter (unknown).\n",
    "- $\\hat{\\theta}$ is the OLS estimator: $\\hat{\\theta} = (X^T X)^{-1} X^T y$.\n",
    "- $y = X\\theta^* + \\varepsilon$.\n",
    "\n",
    "The empirical risk is:\n",
    "$$\n",
    "R_n(\\hat{\\theta}) = \\frac{1}{n} \\| y - X\\hat{\\theta} \\|^2\n",
    "$$\n",
    "Plug in the expressions:\n",
    "$$\n",
    "X\\hat{\\theta} = X(X^T X)^{-1} X^T y\n",
    "$$\n",
    "So,\n",
    "$$\n",
    "y - X\\hat{\\theta} = y - X(X^T X)^{-1} X^T y = (I_n - X(X^T X)^{-1} X^T) y\n",
    "$$\n",
    "Now, substitute $y = X\\theta^* + \\varepsilon$:\n",
    "$$\n",
    "y - X\\hat{\\theta} = (I_n - X(X^T X)^{-1} X^T)(X\\theta^* + \\varepsilon)\n",
    "$$\n",
    "Expand:\n",
    "$$\n",
    "= (I_n - X(X^T X)^{-1} X^T)X\\theta^* + (I_n - X(X^T X)^{-1} X^T)\\varepsilon\n",
    "$$\n",
    "Now, note that:\n",
    "$$\n",
    "(I_n - X(X^T X)^{-1} X^T)X = X - X(X^T X)^{-1} X^T X = X - X = 0\n",
    "$$\n",
    "because $X^T X (X^T X)^{-1} = I_d$.\n",
    "\n",
    "Therefore,\n",
    "$$\n",
    "y - X\\hat{\\theta} = (I_n - X(X^T X)^{-1} X^T)\\varepsilon\n",
    "$$\n",
    "So,\n",
    "$$\n",
    "R_n(\\hat{\\theta}) = \\frac{1}{n} \\| (I_n - X(X^T X)^{-1} X^T)\\varepsilon \\|^2\n",
    "$$\n",
    "Taking the expectation over $\\varepsilon$ gives the result.\n",
    "\n",
    "---\n",
    "\n",
    "## Question 3 (M): Let $A \\in \\mathbb{R}^{n \\times n}$. Show that $\\sum_{i,j} A_{ij}^2 = \\operatorname{tr}(A^T A)$\n",
    "\n",
    "By definition,\n",
    "$$\n",
    "\\operatorname{tr}(A^T A) = \\sum_{k=1}^n (A^T A)_{kk} = \\sum_{k=1}^n \\sum_{l=1}^n A_{lk} A_{kl} = \\sum_{k=1}^n \\sum_{l=1}^n A_{lk} A_{lk} = \\sum_{l=1}^n \\sum_{k=1}^n A_{lk}^2 = \\sum_{i=1}^n \\sum_{j=1}^n A_{ij}^2\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Question 4 (M): Show that $\\mathbb{E}_\\varepsilon \\left[ \\frac{1}{n} \\|A\\varepsilon\\|^2 \\right] = \\frac{\\sigma^2}{n} \\operatorname{tr}(A^T A)$\n",
    "\n",
    "Let $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I_n)$. Then:\n",
    "- $\\mathbb{E}[\\varepsilon] = 0$ (mean zero)\n",
    "- $\\operatorname{Cov}(\\varepsilon) = \\mathbb{E}[(\\varepsilon - \\mathbb{E}[\\varepsilon])(\\varepsilon - \\mathbb{E}[\\varepsilon])^T] = \\sigma^2 I_n$\n",
    "- Thus, $\\mathbb{E}[\\varepsilon \\varepsilon^T] = \\operatorname{Cov}(\\varepsilon) = \\sigma^2 I_n$\n",
    "\n",
    "Now, expand the quadratic form:\n",
    "$$\n",
    "\\|A\\varepsilon\\|^2 = (A\\varepsilon)^T (A\\varepsilon) = \\varepsilon^T A^T A \\varepsilon\n",
    "$$\n",
    "\n",
    "The expectation of a quadratic form is:\n",
    "$$\n",
    "\\mathbb{E}_\\varepsilon[\\varepsilon^T B \\varepsilon] = \\operatorname{tr}(B \\mathbb{E}[\\varepsilon \\varepsilon^T])\n",
    "$$\n",
    "for any symmetric matrix $B$. (Recall: $A^T_{ji} = A_{ij}$.)\n",
    "\n",
    "Apply this to $B = A^T A$:\n",
    "$$\n",
    "\\mathbb{E}_\\varepsilon[\\varepsilon^T A^T A \\varepsilon] = \\operatorname{tr}(A^T A \\mathbb{E}[\\varepsilon \\varepsilon^T]) = \\operatorname{tr}(A^T A \\sigma^2 I_n) = \\sigma^2 \\operatorname{tr}(A^T A)\n",
    "$$\n",
    "\n",
    "So\n",
    "$$\n",
    "\\mathbb{E}_\\varepsilon \\left[ \\frac{1}{n} \\|A\\varepsilon\\|^2 \\right] = \\frac{\\sigma^2}{n} \\operatorname{tr}(A^T A)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Question 5 (M): $A = I_n - X(X^T X)^{-1} X^T$. Show that $A^TA = A$.\n",
    "\n",
    "$A$ is symmetric because $A^T = (I_n - X(X^T X)^{-1} X^T)^T = I_n - X(X^T X)^{-1} X^T = A$.\n",
    "$A$ is idempotent:\n",
    "$$\n",
    "A^2 = (I_n - X(X^T X)^{-1} X^T)^2 = I_n - 2X(X^T X)^{-1} X^T + X(X^T X)^{-1} X^T X(X^T X)^{-1} X^T\n",
    "$$\n",
    "But $X^T X$ is invertible and $X^T X (X^T X)^{-1} = I_d$, so $X^T X(X^T X)^{-1} X^T = X^T$.\n",
    "\n",
    "Thus, $X(X^T X)^{-1} X^T X(X^T X)^{-1} X^T = X(X^T X)^{-1} X^T$.\n",
    "\n",
    "So $A^TA = A^2 = I_n - 2X(X^T X)^{-1} X^T + X(X^T X)^{-1} X^T X(X^T X)^{-1} X^T = A$.\n",
    "\n",
    "---\n",
    "\n",
    "## Question 6 (M): Conclusion: we show that $\\operatorname{tr}(A) = n - d$.\n",
    "\n",
    "Recall $A = I_n - X(X^T X)^{-1} X^T$.\n",
    "\n",
    "First, use the linearity of the trace:\n",
    "$$\n",
    "\\operatorname{tr}(A) = \\operatorname{tr}(I_n) - \\operatorname{tr}(X(X^T X)^{-1} X^T)\n",
    "$$\n",
    "\n",
    "Recall the cyclic property of the trace: $\\operatorname{tr}(AB) = \\operatorname{tr}(BA)$ for compatible matrices $A, B$.\n",
    "\n",
    "So,\n",
    "$$\n",
    "\\operatorname{tr}(X(X^T X)^{-1} X^T) = \\operatorname{tr}((X^T X)^{-1} X^T X) = \\operatorname{tr}(I_d) = d\n",
    "$$\n",
    "\n",
    "Also, $\\operatorname{tr}(I_n) = n$.\n",
    "\n",
    "Therefore,\n",
    "$$\n",
    "\\operatorname{tr}(A) = n - d\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Question 7 (M): What is $\\mathbb{E}\\left[ \\frac{\\|y - X\\hat{\\theta}\\|^2}{n-d} \\right]$?\n",
    "\n",
    "Recall from Question 2 that\n",
    "$$\n",
    "R_n(\\hat{\\theta}) = \\frac{1}{n} \\|y - X\\hat{\\theta}\\|^2\n",
    "$$\n",
    "so\n",
    "$$\n",
    "\\|y - X\\hat{\\theta}\\|^2 = n R_n(\\hat{\\theta})\n",
    "$$\n",
    "From Question 4, we have\n",
    "$$\n",
    "\\mathbb{E}[R_n(\\hat{\\theta})] = \\frac{n-d}{n} \\sigma^2\n",
    "$$\n",
    "Therefore,\n",
    "$$\n",
    "\\mathbb{E}[\\|y - X\\hat{\\theta}\\|^2] = n \\cdot \\mathbb{E}[R_n(\\hat{\\theta})] = n \\cdot \\frac{n-d}{n} \\sigma^2 = (n-d)\\sigma^2\n",
    "$$\n",
    "So,\n",
    "$$\n",
    "\\mathbb{E}\\left[ \\frac{\\|y - X\\hat{\\theta}\\|^2}{n-d} \\right] = \\sigma^2\n",
    "$$\n",
    "\n",
    "Recall: An estimator $T$ of a parameter $\\theta$ is called unbiased if $\\mathbb{E}[T] = \\theta$.\n",
    "\n",
    "**Conclusion:**\n",
    "$\\frac{\\|y - X\\hat{\\theta}\\|^2}{n-d}$ is an unbiased estimator of $\\sigma^2$ in the fixed design OLS setting, because its expectation is exactly $\\sigma^2$."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
